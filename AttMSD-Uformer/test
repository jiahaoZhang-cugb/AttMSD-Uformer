
import os, cv2, glob, numpy as np
import torch
import torch.nn.functional as F
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score
from albumentations import Compose, Resize, Normalize
from albumentations.pytorch import ToTensorV2


test_img_dir = os.path.join(drive_base, '')   
test_mask_dir = os.path.join(drive_base, '')   
save_pred_dir = os.path.join(drive_base, '')   
os.makedirs(save_pred_dir, exist_ok=True)


model = DualEncoderUNetAttMSD().to(device)
model.load_state_dict(torch.load(model_save_path, map_location=device))
model.eval()


transform = Compose([
    Resize(*img_size),
    Normalize(),
    ToTensorV2()
])


y_true_all = []
y_pred_all = []


image_paths = sorted(glob.glob(os.path.join(test_img_dir, '*.png')))
for path in tqdm(image_paths, desc="Testing"):
    name = os.path.basename(path)
    image = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)
    original_size = image.shape[:2][::-1]  
    aug = transform(image=image)
    img_tensor = aug['image'].unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(img_tensor)
        pred = torch.sigmoid(output).squeeze().cpu().numpy()
        pred_mask = (pred > ***).astype(np.uint8)


    save_path = os.path.join(save_pred_dir, name)
    save_mask = (pred_mask * 255).astype(np.uint8)
    save_mask_resized = cv2.resize(save_mask, original_size)  
    cv2.imwrite(save_path, save_mask_resized)


    mask_path = os.path.join(test_mask_dir, name)
    if os.path.exists(mask_path):
        gt_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        gt_mask = cv2.resize(gt_mask, pred_mask.shape[::-1])
        gt_mask = (gt_mask > 0).astype(np.uint8)
        y_true_all.extend(gt_mask.flatten())
        y_pred_all.extend(pred_mask.flatten())


if len(y_true_all) > 0:
    y_true = np.array(y_true_all)
    y_pred = np.array(y_pred_all)
    acc = accuracy_score(y_true, y_pred)
    pre = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    iou = jaccard_score(y_true, y_pred)
    f1  = f1_score(y_true, y_pred)

    print(" Evaluation Metrics:")
    print(f" Accuracy : {acc:.4f}")
    print(f" Precision: {pre:.4f}")
    print(f" Recall   : {rec:.4f}")
    print(f" IoU      : {iou:.4f}")
    print(f" F1-Score : {f1:.4f}")
else:
    print("no")
